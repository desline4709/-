# <center>现代程序设计作业2</center>

## <center>情绪分析</center>

### 一、任务要求

- 数据清洗
- 对一条微博进行情绪分析
- 通过参数控制并返回对应情绪的时间模式
- 通过参数控制并返回对应情绪的空间分布
- 字典方法进行情绪理解的优缺点
- 对情绪的时间和空间分布进行可视化
- 情绪时空模式的管理意义

### 二、任务分块实现情况

本次作业使用函数进行封装，不同任务模块对应不同的函数。

#### 1. 数据加载

封装一个可以加载本地文件的函数进行微博数据和停用词表的加载

```python
def load_data(path, mode=0):
    """
    加载本地数据，只读取txt文本
    :param path: 文件相对位置
    :param mode: 加载文件的模式，0表示正常按行读取，1表示读取后的结果每条去掉末尾的换行符
    :return: 数据列表
    """
    with open(path, 'r', encoding='utf-8') as f:
        if mode == 0:
            data = f.readlines()
        elif mode == 1:
            data = f.read().split()
        else:
            raise Exception('Error mode')
    return data
```

#### 2. 数据清洗

将数据清洗分为以下三个模块封装成函数后，实现数据清洗

##### （1）去除噪声（urls）

通过正则表达式匹配出文本中url的位置，然后将其后的部分去除，得到无噪声的文本

```python
    def remove_urls(wb_data):
        """
        去除微博文本中的URL
        :param wb_data: 微博元数据
        :return: 去除url后的微博数据
        """
        temp = []
        for wb in wb_data:
            try:
                end_index = re.search(r'https?://([\w-]+\.)+[\w-]+(/[\w./?%&=]*)?', wb).span()[0]  # 找到url的起始位置
                temp.append(wb[:end_index])
            except AttributeError:
                temp.append(wb)
        return temp
```

##### （2）分词

利用jieba库内置的函数进行文本分词，同时为了更好地保留情绪标志词，要导入对应的情绪词典

```python
def cutting(wb_data):
    """
    将微博数据进行分词
    :param wb_data: 微博元数据
    :return: 分词后的二维列表
    """

    def init():
        # 加载情绪词典
        jieba.load_userdict("emotion dict\\anger.txt")
        jieba.load_userdict("emotion dict\\disgust.txt")
        jieba.load_userdict("emotion dict\\fear.txt")
        jieba.load_userdict("emotion dict\\joy.txt")
        jieba.load_userdict("emotion dict\\sadness.txt")

    init()
    split_list = []
    for wb in wb_data:
        split_list += [jieba.lcut(wb)]
    return split_list
```

##### （3）过滤停用词

通过停用词表，将分词结果中的停用词过滤掉，得到清洗后的分词结果

```python
    def filtering(wordlist):
        """
        过滤停用词
        :param wordlist: 一条微博的分词列表
        :return: 过滤停用词后的分词列表
        """
        temp = []
        for i in wordlist:
            if i not in stopwords_list:
                temp += [i]
        # print(temp)
        return temp
```

##### （4）本模块全过程

```python
def clean(wb_data, sw_data):
    """
    本函数进行微博语句的清洗
    :param wb_data: 微博分词数据
    :param sw_data: 停用词表
    :return:返回清洗后的分词情况
    """

    def cutting(wb_data):
        """
        将微博数据进行分词
        :param wb_data: 微博元数据
        :return: 分词后的二维列表
        """

        def init():
            # 加载情绪词典
            jieba.load_userdict("emotion dict\\anger.txt")
            jieba.load_userdict("emotion dict\\disgust.txt")
            jieba.load_userdict("emotion dict\\fear.txt")
            jieba.load_userdict("emotion dict\\joy.txt")
            jieba.load_userdict("emotion dict\\sadness.txt")

        init()
        split_list = []
        for wb in wb_data:
            split_list += [jieba.lcut(wb)]
        return split_list

    def remove_urls(wb_data):
        """
        去除微博文本中的URL
        :param wb_data: 微博元数据
        :return: 去除url后的微博数据
        """
        temp = []
        for wb in wb_data:
            try:
                end_index = re.search(r'https?://([\w-]+\.)+[\w-]+(/[\w./?%&=]*)?', wb).span()[0]  # 找到url的起始位置
                temp.append(wb[:end_index])
            except AttributeError:
                temp.append(wb)
        return temp

    def filtering(wordlist):
        """
        过滤停用词
        :param wordlist: 一条微博的分词列表
        :return: 过滤停用词后的分词列表
        """
        temp = []
        for i in wordlist:
            if i not in stopwords_list:
                temp += [i]
        # print(temp)
        return temp

    # 初始化数据结构
    split_list_filtered = []
    stopwords_list = sw_data
    wb_no_urls = remove_urls(wb_data)
    split_list = cutting(wb_no_urls)
    for i in split_list:
        y = filtering(i)
        split_list_filtered += [y]
    # print(split_list_filtered)
    return split_list_filtered
```

