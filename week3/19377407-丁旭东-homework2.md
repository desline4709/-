# <center>现代程序设计作业2</center>

## <center>情绪分析</center>

### 一、任务要求

- 数据清洗
- 对一条微博进行情绪分析
- 通过参数控制并返回对应情绪的时间模式
- 通过参数控制并返回对应情绪的空间分布
- 字典方法进行情绪理解的优缺点
- 对情绪的时间和空间分布进行可视化
- 情绪时空模式的管理意义

### 二、任务分块实现情况

本次作业使用函数进行封装，不同任务模块对应不同的函数。

#### 1. 数据清洗

##### 数据初始化

- 首先读取微博数据：

```python
    with open("weibo.txt", 'r', encoding='utf-8') as f:
        weibolist += f.readlines()
```

- 然后加载情绪词典和停用词表
```python
    # 加载情绪词典
    jieba.load_userdict("emotion dict\\anger.txt")
    jieba.load_userdict("emotion dict\\disgust.txt")
    jieba.load_userdict("emotion dict\\fear.txt")
    jieba.load_userdict("emotion dict\\joy.txt")
    jieba.load_userdict("emotion dict\\sadness.txt")
    # 加载停用词表
    with open("stopwords.txt", "r", encoding='utf-8') as f:
        stopwordslist += f.read().split()
        if " " not in stopwordslist:
            stopwordslist += [' ']
        if "\n" not in stopwordslist:
            stopwordslist += ['\n']
        if "\t" not in stopwordslist:
            stopwordslist += ['\t']
```

将以上部分内容作为一个功能模块进行封装，得到数据清洗中的第一个功能——初始化

```python
    def init(weibolist, stopwordslist):
        # 读取微博数据
        with open("weibo.txt", 'r', encoding='utf-8') as f:
            weibolist += f.readlines()
        # 加载情绪词典
        jieba.load_userdict("emotion dict\\anger.txt")
        jieba.load_userdict("emotion dict\\disgust.txt")
        jieba.load_userdict("emotion dict\\fear.txt")
        jieba.load_userdict("emotion dict\\joy.txt")
        jieba.load_userdict("emotion dict\\sadness.txt")
        # 加载停用词表
        with open("stopwords.txt", "r", encoding='utf-8') as f:
            stopwordslist += f.read().split()
        if " " not in stopwordslist:
            stopwordslist += [' ']
        if "\n" not in stopwordslist:
            stopwordslist += ['\n']
        if "\t" not in stopwordslist:
            stopwordslist += ['\t']
```

##### 分词和过滤

- 然后实现对单个语句进行分词的功能，并封装：

```python
    def cutting(sentence):
        return jieba.lcut(sentence)
```

- 接下来进行单个语句的分词列表进行过滤：

```python
    def filtering(wordlist):
        temp = []
        for i in wordlist:
            if i not in stopwords_list:
                temp += [i]
        return temp
```

##### 总体功能实现

- 最后实现整个数据清洗的功能如下：

```python
# 初始化数据结构
    split_list_filtered = []
    weibo_list = []
    stopwords_list = []
    init(weibo_list, stopwords_list)
    # print(weibo_list, stopwords_list)
    for i in weibo_list:
        x = cutting(i)
        y = filtering(x)
        split_list_filtered += [y]
    # print(split_list_filtered)
```

##### 数据清洗全代码：

```python
def clean():
    """
    本函数进行微博语句的分词，同时清洗分词数据

    :return:
    返回过滤后的分词情况
    """
    def init(weibolist, stopwordslist):
        # 读取微博数据
        with open("weibo.txt", 'r', encoding='utf-8') as f:
            weibolist += f.readlines()
        # 加载情绪词典
        jieba.load_userdict("emotion dict\\anger.txt")
        jieba.load_userdict("emotion dict\\disgust.txt")
        jieba.load_userdict("emotion dict\\fear.txt")
        jieba.load_userdict("emotion dict\\joy.txt")
        jieba.load_userdict("emotion dict\\sadness.txt")
        # 加载停用词表
        with open("stopwords.txt", "r", encoding='utf-8') as f:
            stopwordslist += f.read().split()
        if " " not in stopwordslist:
            stopwordslist += [' ']
        if "\n" not in stopwordslist:
            stopwordslist += ['\n']
        if "\t" not in stopwordslist:
            stopwordslist += ['\t']

    def cutting(sentence):
        return jieba.lcut(sentence)

    def filtering(wordlist):
        temp = []
        for i in wordlist:
            if i not in stopwords_list:
                temp += [i]
        return temp

    # 初始化数据结构
    split_list_filtered = []
    weibo_list = []
    stopwords_list = []
    init(weibo_list, stopwords_list)
    # print(weibo_list, stopwords_list)
    for i in weibo_list:
        x = cutting(i)
        y = filtering(x)
        split_list_filtered += [y]
    # print(split_list_filtered)
    return split_list_filtered
```

#### 二、情绪分析





